# L1 – Standardized Clean Layer

Purpose: Convert raw Chicago crime CSVs (2018→today) into typed, partitioned Parquet for fast, reliable downstream use.

## Data Flow
```
Raw CSVs → L1 Standardization → Partitioned Parquet
data/raw/chicago_crimes_YYYY.csv → data/l1/year=YYYY/month=MM/part-YYYY-MM.parquet
```

## Inputs
- `data/raw/chicago_crimes_YYYY.csv` (from downloader)
- Years: 2018 → current year (2025)
- Raw data contains ~269K records per year

## Outputs
- `data/l1/year=YYYY/month=MM/part-YYYY-MM.parquet`
- Partitioned by year and month for efficient querying
- ~70% smaller file size than CSV format

## Processing Steps (Line by Line)

### 1. **Read Raw CSV**
```python
df = pd.read_csv(year_path)  # Load yearly CSV file
```

### 2. **Column Filtering**
Keep only analytical columns needed for crime analysis:
```python
KEEP_COLUMNS = [
    'id', 'case_number', 'date', 'block', 'iucr', 'primary_type', 
    'description', 'location_description', 'arrest', 'domestic', 
    'beat', 'district', 'ward', 'community_area', 'fbi_code',
    'x_coordinate', 'y_coordinate', 'year', 'updated_on', 
    'latitude', 'longitude'
]
```

### 3. **Type Casting & Schema Enforcement**
```python
CORE_COLUMNS = {
    'id': 'string',
    'case_number': 'string', 
    'date': 'string',
    'arrest': 'boolean',        # True/False for arrests
    'domestic': 'boolean',      # True/False for domestic violence
    'beat': 'Int64',           # Nullable integer (police beat)
    'district': 'Int64',       # Police district number
    'ward': 'Int64',           # City ward number
    'latitude': 'float64',     # Geographic coordinates
    'longitude': 'float64',
    # ... other columns
}
```

### 4. **Date Parsing & Validation**
```python
df['datetime'] = pd.to_datetime(df['date'], errors='coerce')
df = df[~df['datetime'].isna()]  # Drop invalid dates
```

### 5. **Geographic Bounds Filtering**
Remove records outside Chicago city limits:
```python
# Chicago bounds: 41.0-42.5°N, -88.5 to -87.0°W  
lat = df['latitude']
lon = df['longitude']
valid = lat.between(41.0, 42.5) & lon.between(-88.5, -87.0)
df = df[valid]
```

### 6. **Partition Column Creation**
```python
df['year_part'] = df['datetime'].dt.year.astype('int32')
df['month_part'] = df['datetime'].dt.month.astype('int16')
```

### 7. **Partitioned Writing**
```python
for (year, month), group in df.groupby(['year_part', 'month_part']):
    # Write to data/l1/year=YYYY/month=MM/part-YYYY-MM.parquet
    table = pa.Table.from_pandas(group, preserve_index=False)
    pq.write_table(table, output_file)
```

## Data Quality Improvements

### Before L1 (Raw CSV):
- Mixed data types (strings, numbers as text)
- Invalid coordinates (0,0 or outside Chicago)
- Large file sizes (~50MB per year)
- Inconsistent date formats
- Missing value handling unclear

### After L1 (Clean Parquet):
- ✅ **Proper data types**: Booleans, nullable integers, floats
- ✅ **Valid coordinates only**: Within Chicago boundaries
- ✅ **70% smaller files**: Parquet compression (~15MB per year)
- ✅ **Standardized dates**: Parsed datetime objects
- ✅ **Fast querying**: Partitioned by year/month
- ✅ **Missing values**: Properly handled with nullable dtypes

## Performance Benefits

### Storage Efficiency
- **Raw CSV**: ~400MB total (8 years)
- **L1 Parquet**: ~120MB total (70% reduction)
- **Query Speed**: 10x faster for time-range queries

### Memory Usage
- **Columnar storage**: Only load needed columns
- **Efficient dtypes**: Int64 vs object strings
- **Lazy loading**: Process partitions individually

## Validation Metrics

From the test run (2018-2025):
```
2018: 269,136 → 263,559 records (2% invalid coords dropped)
2019: 261,644 → 255,172 records  
2020: 212,634 → 208,067 records (COVID impact visible)
2021: 209,550 → 203,874 records
2022: 239,883 → 234,887 records
2023: 263,086 → 261,245 records
2024: 258,588 → 253,506 records
2025: 151,404 → 151,002 records (partial year)
```

## Run Commands
```bash
# Process all years from 2018 to current
python src/l1_build.py 2018

# Process from specific year  
python src/l1_build.py 2020

# Default starts from 2018
python src/l1_build.py
```

## Error Handling
- **Missing files**: Logs warning, continues with available years
- **Corrupt data**: Drops invalid rows, continues processing
- **Memory issues**: Processes year-by-year to avoid memory overflow
- **I/O errors**: Detailed logging to `logs/l1_build_MMDDYYYY.log`

## Technical Notes
- **Style**: Mirrors `src/data/download_data.py` (standalone, robust logging)
- **Dependencies**: pandas, pyarrow, numpy (no external modules)
- **Partitioning**: Enables efficient time-range filtering in downstream analysis
- **Schema validation**: Ensures consistent data types for ML pipeline

This L1 layer provides the clean, standardized foundation needed for L2 feature engineering and L3 machine learning analysis.
